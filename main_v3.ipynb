{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "from tqdm import tqdm\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MNIST Data Readers (CPU)\n",
    "\n",
    "\n",
    "\n",
    " These functions read the MNIST image and label files from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uint32(b):\n",
    "    return (b[0] << 24) | (b[1] << 16) | (b[2] << 8) | b[3]\n",
    "\n",
    "def read_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        _ = f.read(4)  # magic number\n",
    "        n = make_uint32(f.read(4))\n",
    "        data = np.frombuffer(f.read(n), dtype=np.uint8)\n",
    "    return data\n",
    "\n",
    "def read_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        _ = f.read(4)  # magic number\n",
    "        n = make_uint32(f.read(4))\n",
    "        rows = make_uint32(f.read(4))\n",
    "        cols = make_uint32(f.read(4))\n",
    "        data = np.frombuffer(f.read(n * rows * cols), dtype=np.uint8)\n",
    "        data = data.reshape(n, rows * cols)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Fused GPU Kernels for Neural Network Operations\n",
    "\n",
    "\n",
    "\n",
    " In this section we define fused kernels to perform the forward and backward passes.\n",
    "\n",
    " The forward kernel computes:\n",
    "\n",
    "    z = W * a_in + bias\n",
    "\n",
    "    a = sigmoid(z)\n",
    "\n",
    "\n",
    "\n",
    " The backward kernels compute output errors and backpropagate deltas while fusing weight and bias updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def dsigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1.0 - s)\n",
    "\n",
    "@cuda.jit\n",
    "def forward_layer_kernel(W, a_in, bias, z_out, a_out):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < W.shape[0] and j < a_in.shape[1]:\n",
    "        s = 0.0\n",
    "        for k in range(W.shape[1]):\n",
    "            s += W[i, k] * a_in[k, j]\n",
    "        s += bias[i, 0]  \n",
    "        z_out[i, j] = s\n",
    "        a_out[i, j] = sigmoid(s)\n",
    "\n",
    "@cuda.jit\n",
    "def output_backward_kernel(a, Y, delta):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < a.shape[0] and j < a.shape[1]:\n",
    "        delta[i, j] = (a[i, j] - Y[i, j]) * (a[i, j] * (1.0 - a[i, j]))\n",
    "\n",
    "@cuda.jit\n",
    "def hidden_backward_kernel(W_next, delta_next, z, delta):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < z.shape[0] and j < z.shape[1]:\n",
    "        s = 0.0\n",
    "        for k in range(W_next.shape[0]):\n",
    "            s += W_next[k, i] * delta_next[k, j]\n",
    "        delta[i, j] = s * dsigmoid(z[i, j])\n",
    "\n",
    "@cuda.jit\n",
    "def update_weights_kernel(W, delta, a_prev, learning_rate, minibatch):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < W.shape[0] and j < W.shape[1]:\n",
    "        grad = 0.0\n",
    "        for k in range(delta.shape[1]):\n",
    "            grad += delta[i, k] * a_prev[j, k]\n",
    "        W[i, j] -= learning_rate / minibatch * grad\n",
    "\n",
    "@cuda.jit\n",
    "def update_biases_kernel(bias, delta, learning_rate, minibatch):\n",
    "    i = cuda.grid(1)\n",
    "    if i < bias.shape[0]:\n",
    "        s = 0.0\n",
    "        for k in range(delta.shape[1]):\n",
    "            s += delta[i, k]\n",
    "        bias[i, 0] -= learning_rate / minibatch * s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # GPU Helper Functions for Grid Calculation\n",
    "\n",
    "\n",
    "\n",
    " These functions calculate the grid dimensions for launching 1D and 2D CUDA kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_2d(shape, block=(16, 16)):\n",
    "    gx = (shape[0] + block[0] - 1) // block[0]\n",
    "    gy = (shape[1] + block[1] - 1) // block[1]\n",
    "    return (gx, gy)\n",
    "\n",
    "def grid_1d(n, block=256):\n",
    "    return ((n + block - 1) // block,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Neural Network Definition (Fully on GPU)\n",
    "\n",
    "\n",
    "\n",
    " In this version the networkâ€™s weights, biases, and intermediate results (z, activations, and delta)\n",
    "\n",
    " are stored as GPU device arrays. The forward and backward passes use the fused kernels above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Layer class that holds device arrays for weights, biases, and activations.\n",
    "class LayerGPU:\n",
    "    def __init__(self, n_in, n_out, minibatch):\n",
    "        # Initialize weights with a normal distribution and biases to zeros.\n",
    "        W_host = np.random.normal(0, 1/np.sqrt(n_in), (n_out, n_in)).astype(np.float64)\n",
    "        self.W = cuda.to_device(W_host)\n",
    "        self.b = cuda.to_device(np.zeros((n_out, 1), dtype=np.float64))\n",
    "        # Allocate device memory for z (pre-activation), a (activation), and delta (error)\n",
    "        self.z = cuda.device_array((n_out, minibatch), dtype=np.float64)\n",
    "        self.a = cuda.device_array((n_out, minibatch), dtype=np.float64)\n",
    "        self.delta = cuda.device_array((n_out, minibatch), dtype=np.float64)\n",
    "\n",
    "# Define the ANN class that uses the GPU-based layers.\n",
    "class ANN:\n",
    "    def __init__(self, layer_sizes, alpha, minibatch):\n",
    "        \"\"\"\n",
    "        layer_sizes: list of layer sizes, e.g. [784, 30, 10]\n",
    "        alpha: learning rate\n",
    "        minibatch: batch size\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.minibatch = minibatch\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.layers = []\n",
    "        # Create layers for each connection (input->hidden, hidden->output, etc.)\n",
    "        for i in range(self.n_layers - 1):\n",
    "            n_in = layer_sizes[i]\n",
    "            n_out = layer_sizes[i+1]\n",
    "            self.layers.append(LayerGPU(n_in, n_out, minibatch))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass: X is a host array of shape (n_in, batch_size).\n",
    "        The input is copied to the GPU once, and each layer is processed with a fused kernel.\n",
    "        Returns the final output as a host array.\n",
    "        \"\"\"\n",
    "        a_in = cuda.to_device(X)\n",
    "        block = (16, 16)\n",
    "        for layer in self.layers:\n",
    "            grid = grid_2d(layer.z.shape, block)\n",
    "            forward_layer_kernel[grid, block](layer.W, a_in, layer.b, layer.z, layer.a)\n",
    "            a_in = layer.a  # Activation becomes input for next layer\n",
    "        out = a_in.copy_to_host()\n",
    "        return out\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Backward pass: X (host, shape (n_in, batch_size)) and Y (host one-hot, shape (n_out, batch_size)).\n",
    "        Gradients are computed and the weights/biases are updated on the GPU.\n",
    "        \"\"\"\n",
    "        block = (16, 16)\n",
    "        # Process output layer: compute delta = (a - Y) * (a*(1-a))\n",
    "        output_layer = self.layers[-1]\n",
    "        Y_dev = cuda.to_device(Y)\n",
    "        grid = grid_2d(output_layer.a.shape, block)\n",
    "        output_backward_kernel[grid, block](output_layer.a, Y_dev, output_layer.delta)\n",
    "        \n",
    "        # Backpropagate through hidden layers\n",
    "        for l in range(len(self.layers) - 2, -1, -1):\n",
    "            current_layer = self.layers[l]\n",
    "            next_layer = self.layers[l+1]\n",
    "            grid = grid_2d(current_layer.a.shape, block)\n",
    "            hidden_backward_kernel[grid, block](next_layer.W, next_layer.delta, current_layer.z, current_layer.delta)\n",
    "        \n",
    "        # Update weights and biases for each layer\n",
    "        a_prev = cuda.to_device(X)  # For first layer, a_prev is the input\n",
    "        for layer in self.layers:\n",
    "            grid_w = grid_2d(layer.W.shape, block)\n",
    "            update_weights_kernel[grid_w, block](layer.W, layer.delta, a_prev, self.alpha, self.minibatch)\n",
    "            grid_b = grid_1d(layer.b.shape[0], block=256)\n",
    "            update_biases_kernel[grid_b, 256](layer.b, layer.delta, self.alpha, self.minibatch)\n",
    "            a_prev = layer.a\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict labels for input X by performing a forward pass and then returning the argmax.\n",
    "        \"\"\"\n",
    "        out = self.forward(X)\n",
    "        return np.argmax(out, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training and Testing Helpers (CPU)\n",
    "\n",
    "\n",
    "\n",
    " These functions create one-hot encodings, compute cross-entropy loss, and evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, n_classes=10):\n",
    "    arr = np.zeros((n_classes, len(labels)), dtype=np.float64)\n",
    "    arr[labels, np.arange(len(labels))] = 1.0\n",
    "    return arr\n",
    "\n",
    "def cross_entropy_cpu(predictions, targets):\n",
    "    eps = 1e-12\n",
    "    clipped = np.clip(predictions, eps, 1 - eps)\n",
    "    return -np.sum(targets * np.log(clipped)) / predictions.shape[1]\n",
    "\n",
    "def accuracy(model, X, labels):\n",
    "    \"\"\"\n",
    "    Evaluate classification accuracy.\n",
    "    X: host array of shape (n_in, num_samples)\n",
    "    labels: array of integer labels.\n",
    "    \"\"\"\n",
    "    batch_size = model.minibatch\n",
    "    num_samples = X.shape[1]\n",
    "    correct = 0\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        x_batch = X[:, i:i+batch_size]\n",
    "        preds = model.predict(x_batch)\n",
    "        correct += np.sum(preds == labels[i:i+batch_size])\n",
    "    return correct / num_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Main Training and Testing Loop\n",
    "\n",
    "\n",
    "\n",
    " This section loads the MNIST dataset, normalizes the data, creates the network,\n",
    "\n",
    " and trains it for several epochs. Note that only the input minibatch and targets are transferred\n",
    "\n",
    " to the GPU each iteration. All other computations remain on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Acc: 10.37%: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [00:10<00:00, 366.63it/s]\n",
      "Epoch 2 - Acc: 74.34%: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [00:10<00:00, 372.81it/s]\n",
      "Epoch 3 - Acc: 87.02%: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [00:09<00:00, 401.25it/s]\n",
      "Epoch 4 - Acc: 88.76%: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [00:09<00:00, 397.13it/s]\n",
      "Epoch 5 - Acc: 89.76%: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [00:10<00:00, 375.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model : Accuracy = 90.43%, Cross-Entropy Error = 0.4456\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"DATA\"\n",
    "train_img = read_images(os.path.join(DATA_PATH, \"train-images.idx3-ubyte\"))\n",
    "train_label = read_labels(os.path.join(DATA_PATH, \"train-labels.idx1-ubyte\"))\n",
    "test_img = read_images(os.path.join(DATA_PATH, \"t10k-images.idx3-ubyte\"))\n",
    "test_label = read_labels(os.path.join(DATA_PATH, \"t10k-labels.idx1-ubyte\"))\n",
    "\n",
    "train_img = train_img.astype(np.float64) / 255.0\n",
    "test_img = test_img.astype(np.float64) / 255.0\n",
    "\n",
    "alpha = 0.05\n",
    "batch_size = 16\n",
    "layer_sizes = [784, 30, 10]\n",
    "epochs = 5\n",
    "\n",
    "net = ANN(layer_sizes, alpha, batch_size)\n",
    "\n",
    "Xtest = test_img.T\n",
    "Ytest = test_label\n",
    "\n",
    "init_acc = accuracy(net, Xtest, Ytest)\n",
    "\n",
    "N = train_img.shape[0]\n",
    "for ep in range(epochs):\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "    ce_total = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    batch_iter = range(0, N - batch_size + 1, batch_size)\n",
    "    acc = accuracy(net, Xtest, Ytest)\n",
    "    desc = f'Epoch {ep+1} - Acc: {acc*100:.2f}%'\n",
    "\n",
    "    for i in tqdm(batch_iter, desc=desc):\n",
    "        bidx = idx[i:i+batch_size]\n",
    "        x_b = train_img[bidx].T\n",
    "        y_b = one_hot(train_label[bidx], 10)\n",
    "\n",
    "        out = net.forward(x_b)\n",
    "        ce = cross_entropy_cpu(out, y_b)\n",
    "        ce_total += ce\n",
    "\n",
    "        net.backward(x_b, y_b)\n",
    "        n_batches += 1\n",
    "\n",
    "    ce_mean = ce_total / n_batches\n",
    "    acc = accuracy(net, Xtest, Ytest)\n",
    "\n",
    "final_ce = ce_total / n_batches\n",
    "final_acc = accuracy(net, Xtest, Ytest)\n",
    "print(\"Final Model : Accuracy = {:.2f}%, Cross-Entropy Error = {:.4f}\".format(final_acc*100, final_ce))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECN_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
