{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468f6b88",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Lecture des fichiers MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uint32(byte_array):\n",
    "    \"\"\" Recompose un entier 32 bits à partir de 4 octets de poids fort à poids faible \"\"\"\n",
    "    return ((byte_array[0] << 24) \n",
    "          | (byte_array[1] << 16) \n",
    "          | (byte_array[2] <<  8) \n",
    "          | (byte_array[3] <<  0))\n",
    "\n",
    "def read_labels(filename):\n",
    "    \"\"\" Lit un fichier de labels MNIST \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        _ = f.read(4)  # Magic number (non utilisé)\n",
    "        n_bytes = f.read(4)\n",
    "        n = make_uint32(n_bytes)\n",
    "        labels = np.frombuffer(f.read(n), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "def read_images(filename):\n",
    "    \"\"\" Lit un fichier d'images MNIST \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        _ = f.read(4)  # Magic number (non utilisé)\n",
    "        n_bytes = f.read(4)\n",
    "        n = make_uint32(n_bytes)\n",
    "        row_bytes = f.read(4)\n",
    "        col_bytes = f.read(4)\n",
    "        rows = make_uint32(row_bytes)\n",
    "        cols = make_uint32(col_bytes)\n",
    "        images_raw = f.read(n * rows * cols)\n",
    "        images = np.frombuffer(images_raw, dtype=np.uint8)\n",
    "        images = images.reshape(n, rows * cols)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720cddeb",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b06df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_to_n(n):\n",
    "    \"\"\" Crée un tableau [0, 1, 2, ..., n-1] \"\"\"\n",
    "    return np.arange(n, dtype=np.uint32)\n",
    "\n",
    "def shuffle(t, number_of_switch):\n",
    "    \"\"\" Mélange un tableau t aléatoirement, en réalisant 'number_of_switch' échanges \"\"\"\n",
    "    size = len(t)\n",
    "    for _ in range(number_of_switch):\n",
    "        x = np.random.randint(0, size)\n",
    "        y = np.random.randint(0, size)\n",
    "        tmp = t[x]\n",
    "        t[x] = t[y]\n",
    "        t[y] = tmp\n",
    "        \n",
    "def init_sigma(nneurons_prev):\n",
    "    return 1.0 / np.sqrt(nneurons_prev)  # REPLACE NP - MATH\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\" Fonction d'activation vectorisée \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    \"\"\" Dérivée de sigmoid vectorisée \"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1.0 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a19510",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Fonctions Matricielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12954ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alloc_matrix(rows, columns):\n",
    "    # En C, on fait un malloc puis un tableau 1D rows*columns.\n",
    "    # En Python, on crée un np.ndarray de shape (rows, columns).\n",
    "    return np.zeros((rows, columns), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff72f76",
   "metadata": {},
   "source": [
    "### 3.1. Version Naïve\n",
    "(Utilise des boucles explicites pour imiter l'implémentation en C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a31fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_dot_naive(m1, m2):\n",
    "    # Produit entre deux matrices : triple boucle\n",
    "    r1, c1 = m1.shape\n",
    "    r2, c2 = m2.shape\n",
    "    res = alloc_matrix(r1, c2)\n",
    "    for i in range(r1):\n",
    "        for j in range(c2):\n",
    "            tmp = 0.0\n",
    "            for k in range(c1):\n",
    "                tmp += m1[i, k] * m2[k, j]\n",
    "            res[i, j] = tmp\n",
    "    return res\n",
    "\n",
    "def matrix_sum_naive(m1, m2):\n",
    "    # Addition entre deux matrices : double boucle\n",
    "    r, c = m1.shape\n",
    "    res = alloc_matrix(r, c)\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            res[i, j] = m1[i, j] + m2[i, j]\n",
    "    return res\n",
    "\n",
    "def matrix_minus_naive(m1, m2):\n",
    "    # Soustraction entre deux matrices (m1 - m2) : double boucle\n",
    "    r, c = m1.shape\n",
    "    res = alloc_matrix(r, c)\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            res[i, j] = m1[i, j] - m2[i, j]\n",
    "    return res\n",
    "\n",
    "def hadamard_product_naive(m1, m2):\n",
    "    # Produit d'Hadamard : double boucle\n",
    "    r, c = m1.shape\n",
    "    res = alloc_matrix(r, c)\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            res[i, j] = m1[i, j] * m2[i, j]\n",
    "    return res\n",
    "\n",
    "def matrix_function_naive(m1, func):\n",
    "    # Applique une fonction à tous les éléments : double boucle\n",
    "    r, c = m1.shape\n",
    "    res = alloc_matrix(r, c)\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            res[i, j] = func(m1[i, j])\n",
    "    return res\n",
    "\n",
    "def matrix_transpose_naive(m):\n",
    "    # Transposition d'une matrice : double boucle\n",
    "    r, c = m.shape\n",
    "    res = alloc_matrix(c, r)\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            res[j, i] = m[i, j]\n",
    "    return res\n",
    "\n",
    "def matrix_scalar_naive(m, s):\n",
    "    # Multiplication par un scalaire : double boucle\n",
    "    r, c = m.shape\n",
    "    res = alloc_matrix(r, c)\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            res[i, j] = m[i, j] * s\n",
    "    return res\n",
    "\n",
    "def matrix_memcpy_naive(dest, src):\n",
    "    # Copie élément par élément de src dans dest : double boucle\n",
    "    r, c = src.shape\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            dest[i, j] = src[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae5515",
   "metadata": {},
   "source": [
    "### 3.2. Version Numpy\n",
    "(Utilise les opérations vectorisées de NumPy pour une exécution rapide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_dot_numpy(m1, m2):\n",
    "    return np.dot(m1, m2)\n",
    "\n",
    "def matrix_sum_numpy(m1, m2):\n",
    "    return m1 + m2\n",
    "\n",
    "def matrix_minus_numpy(m1, m2):\n",
    "    return m1 - m2\n",
    "\n",
    "def hadamard_product_numpy(m1, m2):\n",
    "    return m1 * m2\n",
    "\n",
    "def matrix_function_numpy(m1, func):\n",
    "    # On suppose que func est déjà vectorisée\n",
    "    return func(m1)\n",
    "\n",
    "def matrix_transpose_numpy(m):\n",
    "    return m.T\n",
    "\n",
    "def matrix_scalar_numpy(m, s):\n",
    "    return m * s\n",
    "\n",
    "def matrix_memcpy_numpy(dest, src):\n",
    "    np.copyto(dest, src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e5923",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Réseau de Neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMPLACER NUMPY PAR NAIVE\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, layer_number, number_of_neurons, nneurons_previous_layer, minibatch_size):\n",
    "        self.number_of_neurons = number_of_neurons\n",
    "        self.minibatch_size = minibatch_size\n",
    "        # activations et z de shape (nneurons, batch_size)\n",
    "        self.activations = alloc_matrix(number_of_neurons, minibatch_size)\n",
    "        self.z           = alloc_matrix(number_of_neurons, minibatch_size)\n",
    "        self.delta       = alloc_matrix(number_of_neurons, minibatch_size)\n",
    "        \n",
    "        # weights de shape (nneurons, nneurons_previous_layer)\n",
    "        self.weights     = alloc_matrix(number_of_neurons, nneurons_previous_layer)\n",
    "        # biases de shape (nneurons, 1)\n",
    "        self.biases      = alloc_matrix(number_of_neurons, 1)\n",
    "        \n",
    "        if layer_number > 0:\n",
    "            self.init_weight(nneurons_previous_layer)\n",
    "    \n",
    "    def init_weight(self, nneurons_prev):\n",
    "        # Initialisation vectorisée\n",
    "        sigma = init_sigma(nneurons_prev)\n",
    "        r, c = self.weights.shape\n",
    "        self.weights = np.random.normal(0.0, sigma, size=(r, c))\n",
    "\n",
    "class ANN:\n",
    "    def __init__(self, alpha, minibatch_size, number_of_layers, nneurons_per_layer):\n",
    "        self.alpha = alpha\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.number_of_layers = number_of_layers\n",
    "        self.layers = []\n",
    "        for i in range(number_of_layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(\n",
    "                    Layer(i, nneurons_per_layer[i], \n",
    "                          nneurons_per_layer[i],  \n",
    "                          minibatch_size)\n",
    "                )\n",
    "            else:\n",
    "                self.layers.append(\n",
    "                    Layer(i, nneurons_per_layer[i],\n",
    "                          nneurons_per_layer[i-1],\n",
    "                          minibatch_size)\n",
    "                )\n",
    "\n",
    "def set_input(nn, input_matrix):\n",
    "    # Utilise la version numpy pour la rapidité\n",
    "    matrix_memcpy_naive(nn.layers[0].activations, input_matrix)\n",
    "\n",
    "def forward(nn, activation_function):\n",
    "    for l in range(1, nn.number_of_layers):\n",
    "        layer_l = nn.layers[l]\n",
    "        layer_prev = nn.layers[l-1]\n",
    "        z1 = matrix_dot_naive(layer_l.weights, layer_prev.activations)\n",
    "        ones = np.ones((1, nn.minibatch_size), dtype=np.float64)\n",
    "        z2 = matrix_dot_naive(layer_l.biases, ones)\n",
    "        layer_l.z = matrix_sum_naive(z1, z2)\n",
    "        layer_l.activations = matrix_function_naive(layer_l.z, activation_function)\n",
    "\n",
    "def backward(nn, y, derivative_actfunct):\n",
    "    L = nn.number_of_layers - 1\n",
    "    layer_L = nn.layers[L]\n",
    "    tmp = matrix_minus_naive(layer_L.activations, y)\n",
    "    dfzL = matrix_function_naive(layer_L.z, derivative_actfunct)\n",
    "    layer_L.delta = hadamard_product_naive(tmp, dfzL)\n",
    "    for l in range(L, 1, -1):\n",
    "        layer_l = nn.layers[l]\n",
    "        layer_lm1 = nn.layers[l-1]\n",
    "        w_l_transp = matrix_transpose_naive(layer_l.weights)\n",
    "        delta_tmp = matrix_dot_naive(w_l_transp, layer_l.delta)\n",
    "        dfz = matrix_function_naive(layer_lm1.z, derivative_actfunct)\n",
    "        layer_lm1.delta = hadamard_product_naive(delta_tmp, dfz)\n",
    "    for l in range(1, nn.number_of_layers):\n",
    "        layer_l = nn.layers[l]\n",
    "        layer_lm1 = nn.layers[l-1]\n",
    "        a_lm1_transp = matrix_transpose_naive(layer_lm1.activations)\n",
    "        w1 = matrix_dot_naive(layer_l.delta, a_lm1_transp)\n",
    "        w1 = matrix_scalar_naive(w1, nn.alpha / nn.minibatch_size)\n",
    "        layer_l.weights = matrix_minus_naive(layer_l.weights, w1)\n",
    "        ones = np.ones((nn.minibatch_size, 1), dtype=np.float64)\n",
    "        b1 = matrix_dot_naive(layer_l.delta, ones)\n",
    "        b1 = matrix_scalar_naive(b1, nn.alpha / nn.minibatch_size)\n",
    "        layer_l.biases = matrix_minus_naive(layer_l.biases, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465923e",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Fonctions d'entraînement\n",
    "(Les versions Naïve et Numpy sont toutes deux fournies pour comparer les performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610cbfd7",
   "metadata": {},
   "source": [
    "### 5.1. Version Naïve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afccb59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_naive(x, y, minibatch_idx, train_img, train_label):\n",
    "    \"\"\"\n",
    "    Remplit les matrices x et y avec le mini-batch de manière naïve (boucles explicites).\n",
    "    x -> shape (784, minibatch_size)\n",
    "    y -> shape (10, minibatch_size)\n",
    "    \"\"\"\n",
    "    batch_size = len(minibatch_idx)\n",
    "    for col, idx in enumerate(minibatch_idx):\n",
    "        for row in range(784):\n",
    "            x[row, col] = train_img[idx, row] / 255.0\n",
    "        for row in range(10):\n",
    "            y[row, col] = 0.0\n",
    "        true_label = train_label[idx]\n",
    "        y[true_label, col] = 1.0\n",
    "\n",
    "def accuracy_naive(nn, test_img, test_label, minibatch_size):\n",
    "    \"\"\"\n",
    "    Calcule l'accuracy en pourcentage sur l'ensemble des données test en mode naïf.\n",
    "    On itère par mini-batch et utilise des boucles explicites pour déterminer les prédictions.\n",
    "    \"\"\"\n",
    "    ntest = test_img.shape[0]\n",
    "    good = 0\n",
    "    idxs = zero_to_n(ntest)\n",
    "    x = alloc_matrix(784, minibatch_size)\n",
    "    y = alloc_matrix(10, minibatch_size)\n",
    "    nbatches = (ntest // minibatch_size) * minibatch_size\n",
    "    for i in range(0, nbatches, minibatch_size):\n",
    "        batch_indices = idxs[i:i+minibatch_size]\n",
    "        populate_naive(x, y, batch_indices, test_img, test_label)\n",
    "        set_input(nn, x)\n",
    "        forward(nn, sigmoid)\n",
    "        last_activ = nn.layers[-1].activations\n",
    "        for col in range(minibatch_size):\n",
    "            max_val = -1e9\n",
    "            max_idx = 0\n",
    "            for row in range(10):\n",
    "                val = last_activ[row, col]\n",
    "                if val > max_val:\n",
    "                    max_val = val\n",
    "                    max_idx = row\n",
    "            if max_idx == test_label[i + col]:\n",
    "                good += 1\n",
    "    return (100.0 * good) / nbatches\n",
    "\n",
    "def cross_entropy_naive(y_pred, y_true, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Calcule la cross-entropy moyenne pour un mini-batch en mode naïf (boucles explicites).\n",
    "    \"\"\"\n",
    "    batch_size = y_true.shape[1]\n",
    "    ce_sum = 0.0\n",
    "    for col in range(batch_size):\n",
    "        for row in range(10):\n",
    "            val_pred = y_pred[row, col]\n",
    "            val_true = y_true[row, col]\n",
    "            clipped = max(min(val_pred, 1.0 - eps), eps)\n",
    "            if val_true > 0.0:\n",
    "                ce_sum -= math.log(clipped)\n",
    "    return ce_sum / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab3ed3",
   "metadata": {},
   "source": [
    "### 5.2. Version Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_numpy(x, y, minibatch_idx, train_img, train_label):\n",
    "    \"\"\"\n",
    "    Remplit les matrices x et y avec le mini-batch en utilisant des opérations vectorisées.\n",
    "    x -> shape (784, minibatch_size)\n",
    "    y -> shape (10, minibatch_size)\n",
    "    \"\"\"\n",
    "    x_batch = train_img[minibatch_idx].astype(np.float64) / 255.0\n",
    "    matrix_memcpy_numpy(x, x_batch.T)  # Transpose pour avoir (784, batch_size)\n",
    "    y.fill(0.0)\n",
    "    indices = train_label[minibatch_idx]\n",
    "    y[indices, np.arange(len(minibatch_idx))] = 1.0\n",
    "\n",
    "def accuracy_numpy(nn, test_img, test_label, minibatch_size):\n",
    "    \"\"\"\n",
    "    Compute the accuracy (%) on the test set using fully vectorized NumPy operations.\n",
    "    Processes the test set in mini-batches.\n",
    "    \"\"\"\n",
    "    ntest = test_img.shape[0]\n",
    "    nbatches = (ntest // minibatch_size) * minibatch_size\n",
    "    correct = 0\n",
    "    for i in range(0, nbatches, minibatch_size):\n",
    "        batch_indices = np.arange(i, i + minibatch_size)\n",
    "        x = test_img[batch_indices].T.astype(np.float64) / 255.0\n",
    "        set_input(nn, x)\n",
    "        forward(nn, sigmoid)\n",
    "        preds = np.argmax(nn.layers[-1].activations, axis=0)\n",
    "        correct += np.sum(preds == test_label[batch_indices])\n",
    "    return (100.0 * correct) / nbatches\n",
    "\n",
    "def cross_entropy_numpy(y_pred, y_true, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy error for a mini-batch using vectorized NumPy operations.\n",
    "    \"\"\"\n",
    "    y_pred = np.clip(y_pred, eps, 1.0 - eps)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd75eef",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Exécution Principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMPLACER NUMPY PAR NAIVE\n",
    "\n",
    "DATA_PATH = \"DATA\"\n",
    "\n",
    "# 6.1) Lecture des données MNIST\n",
    "train_img = read_images(os.path.join(DATA_PATH, \"train-images.idx3-ubyte\"))\n",
    "train_label = read_labels(os.path.join(DATA_PATH, \"train-labels.idx1-ubyte\"))\n",
    "test_img = read_images(os.path.join(DATA_PATH, \"t10k-images.idx3-ubyte\"))\n",
    "test_label = read_labels(os.path.join(DATA_PATH, \"t10k-labels.idx1-ubyte\"))\n",
    "\n",
    "datasize = train_img.shape[0]\n",
    "ntest = test_img.shape[0]\n",
    "\n",
    "alpha = 0.05\n",
    "minibatch_size = 16\n",
    "number_of_layers = 3\n",
    "nneurons_per_layer = [784, 30, 10]  # 28*28 = 784\n",
    "nn = ANN(alpha, minibatch_size, number_of_layers, nneurons_per_layer)\n",
    "\n",
    "shuffled_idx = zero_to_n(datasize)\n",
    "x = alloc_matrix(784, minibatch_size)\n",
    "y = alloc_matrix(10, minibatch_size)\n",
    "\n",
    "# Using the numpy version for fast execution:\n",
    "acc_start = accuracy_naive(nn, test_img, test_label, minibatch_size)\n",
    "print(\"Starting accuracy:\", acc_start)\n",
    "\n",
    "NEPOCHS = 20\n",
    "for epoch in range(NEPOCHS):\n",
    "    shuffle(shuffled_idx, datasize)\n",
    "    nbatches = (datasize // minibatch_size) * minibatch_size\n",
    "    batch_iter = range(0, nbatches, minibatch_size)\n",
    "    ce_total = 0.0\n",
    "    n_train_batches = 0\n",
    "    acc = accuracy_naive(nn, test_img, test_label, minibatch_size)\n",
    "    desc = f'Epoch {epoch} - Acc: {acc:.2f}%'\n",
    "    for i in tqdm(batch_iter, desc=desc):\n",
    "        batch_indices = shuffled_idx[i : i + minibatch_size]\n",
    "        # To test the numpy version, call populate_naive.\n",
    "        populate_naive(x, y, batch_indices, train_img, train_label)\n",
    "        set_input(nn, x)\n",
    "        forward(nn, sigmoid)\n",
    "        y_pred = nn.layers[-1].activations  \n",
    "        ce_batch = cross_entropy_naive(y_pred, y)\n",
    "        ce_total += ce_batch\n",
    "        n_train_batches += 1\n",
    "        backward(nn, y, dsigmoid)\n",
    "    ce_mean = ce_total / n_train_batches\n",
    "    acc = accuracy_naive(nn, test_img, test_label, minibatch_size)\n",
    "    desc = f'Epoch {epoch} - Acc: {acc:.2f}%, CE: {ce_mean:.4f}'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECN_GPU",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
