{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Lecture des fichiers MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uint32(byte_array):\n",
    "    # Recompose un entier 32 bits à partir de 4 octets \n",
    "    # de poids fort à poids faible\n",
    "    return ((byte_array[0] << 24) \n",
    "          | (byte_array[1] << 16) \n",
    "          | (byte_array[2] <<  8) \n",
    "          | (byte_array[3] <<  0))\n",
    "\n",
    "def read_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Magic number (4 octets) – non utilisé ici\n",
    "        _ = f.read(4)\n",
    "        \n",
    "        # Nombre d'étiquettes\n",
    "        n_bytes = f.read(4)\n",
    "        n = make_uint32(n_bytes)\n",
    "\n",
    "        # Lecture des labels\n",
    "        labels = np.frombuffer(f.read(n), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "def read_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Magic number (4 octets) – non utilisé ici\n",
    "        _ = f.read(4)\n",
    "        \n",
    "        # Nombre d'images\n",
    "        n_bytes = f.read(4)\n",
    "        n = make_uint32(n_bytes)\n",
    "\n",
    "        # Nombre de lignes et de colonnes (ici 28x28)\n",
    "        row_bytes = f.read(4)\n",
    "        col_bytes = f.read(4)\n",
    "        rows = make_uint32(row_bytes)\n",
    "        cols = make_uint32(col_bytes)\n",
    "        \n",
    "        # Lecture des pixels (chacun sur 1 octet)\n",
    "        images_raw = f.read(n * rows * cols)\n",
    "        # On crée un tableau NumPy de shape (n, rows*cols)\n",
    "        # pour être cohérent avec le C qui stockait chaque image \n",
    "        # en un tableau 1D de 28*28\n",
    "        images = np.frombuffer(images_raw, dtype=np.uint8)\n",
    "        images = images.reshape(n, rows * cols)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_to_n(n):\n",
    "    \"\"\"\n",
    "    Crée un tableau [0, 1, 2, ..., n-1].\n",
    "    \"\"\"\n",
    "    return np.arange(n, dtype=np.uint32)\n",
    "\n",
    "def shuffle(t, number_of_switch):\n",
    "    \"\"\"\n",
    "    Mélange un tableau t aléatoirement, \n",
    "    en réalisant 'number_of_switch' échanges.\n",
    "    \"\"\"\n",
    "    # Dans le code C, le mélange se fait en échangeant\n",
    "    # aléatoirement number_of_switch fois.\n",
    "    size = len(t)\n",
    "    for _ in range(number_of_switch):\n",
    "        x = np.random.randint(0, size)\n",
    "        y = np.random.randint(0, size)\n",
    "        tmp = t[x]\n",
    "        t[x] = t[y]\n",
    "        t[y] = tmp\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    # Dérivée de sigmoid\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def cross_entropy_error(y_pred, y_true, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Calcule la cross-entropy moyenne pour un mini-batch.\n",
    "    y_pred et y_true sont de shape (10, batch_size).\n",
    "    \"\"\"\n",
    "    # On s'assure d'éviter log(0)\n",
    "    clipped = np.clip(y_pred, eps, 1.0 - eps)\n",
    "    batch_size = y_true.shape[1]\n",
    "    # somme de - y_true * log(y_pred) sur toutes les classes, \n",
    "    # puis moyenne sur le batch\n",
    "    ce = - np.sum(y_true * np.log(clipped)) / batch_size\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alloc_matrix(rows, columns):\n",
    "    # En C, on faisait un malloc puis un tableau 1D rows*columns.\n",
    "    # En Python, on retournera un np.ndarray de shape (rows, columns).\n",
    "    return np.zeros((rows, columns), dtype=np.float64)\n",
    "\n",
    "def matrix_dot(m1, m2):\n",
    "    # m1 shape = (rows1, col1)\n",
    "    # m2 shape = (rows2, col2)\n",
    "    # assert col1 == rows2\n",
    "    return np.dot(m1, m2)\n",
    "\n",
    "def matrix_sum(m1, m2):\n",
    "    return m1 + m2\n",
    "\n",
    "def matrix_minus(m1, m2):\n",
    "    return m1 - m2\n",
    "\n",
    "def hadamard_product(m1, m2):\n",
    "    return m1 * m2\n",
    "\n",
    "def matrix_function(m1, func):\n",
    "    # Applique f à chaque élément\n",
    "    return func(m1)\n",
    "\n",
    "def matrix_transpose(m):\n",
    "    return m.T\n",
    "\n",
    "def matrix_scalar(m, s):\n",
    "    return m * s\n",
    "\n",
    "def matrix_memcpy(dest, src):\n",
    "    # recopie le contenu de src dans dest, supposé de la même shape\n",
    "    np.copyto(dest, src)\n",
    "\n",
    "#\n",
    "# Définitions des couches et du réseau\n",
    "#\n",
    "class Layer:\n",
    "    def __init__(self, layer_number, number_of_neurons, nneurons_previous_layer, minibatch_size):\n",
    "        self.number_of_neurons = number_of_neurons\n",
    "        self.minibatch_size = minibatch_size\n",
    "        # Matrices\n",
    "        # activations et z de shape (nneurons, batch_size)\n",
    "        self.activations = alloc_matrix(number_of_neurons, minibatch_size)\n",
    "        self.z           = alloc_matrix(number_of_neurons, minibatch_size)\n",
    "        self.delta       = alloc_matrix(number_of_neurons, minibatch_size)\n",
    "        \n",
    "        # weights de shape (nneurons, nneurons_previous_layer)\n",
    "        self.weights     = alloc_matrix(number_of_neurons, nneurons_previous_layer)\n",
    "        # biases de shape (nneurons, 1)\n",
    "        self.biases      = alloc_matrix(number_of_neurons, 1)\n",
    "        \n",
    "        # Initialisation des poids si ce n'est pas la couche d'entrée\n",
    "        if layer_number > 0:\n",
    "            self.init_weight(nneurons_previous_layer)\n",
    "    \n",
    "    def init_weight(self, nneurons_prev):\n",
    "        # Equivalent de init_weight() en C (Xavier init approchée, via normalRand)\n",
    "        # On utilise np.random.normal(0, 1/sqrt(nneurons_prev), ...)\n",
    "        sigma = 1.0 / math.sqrt(nneurons_prev)\n",
    "        self.weights[:] = np.random.normal(0.0, sigma, size=self.weights.shape)\n",
    "        \n",
    "class ANN:\n",
    "    def __init__(self, alpha, minibatch_size, number_of_layers, nneurons_per_layer):\n",
    "        self.alpha = alpha\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.number_of_layers = number_of_layers\n",
    "        \n",
    "        # Création des couches\n",
    "        self.layers = []\n",
    "        for i in range(number_of_layers):\n",
    "            if i == 0:\n",
    "                # Couche d'entrée\n",
    "                self.layers.append(\n",
    "                    Layer(i, nneurons_per_layer[i], \n",
    "                          nneurons_per_layer[i],  # la taille \"précédente\" n'a pas d'impact pour la couche d'entrée\n",
    "                          minibatch_size)\n",
    "                )\n",
    "            else:\n",
    "                # Couches cachées / sortie\n",
    "                self.layers.append(\n",
    "                    Layer(i, nneurons_per_layer[i],\n",
    "                          nneurons_per_layer[i-1],\n",
    "                          minibatch_size)\n",
    "                )\n",
    "\n",
    "def set_input(nn, input_matrix):\n",
    "    # Recopie input_matrix dans la \"couche 0\" (couche d'entrée)\n",
    "    matrix_memcpy(nn.layers[0].activations, input_matrix)\n",
    "\n",
    "def forward(nn, activation_function):\n",
    "    # On parcourt les couches de 1 à L-1 en faisant:\n",
    "    # z^l = w^l . a^(l-1) + b^l\n",
    "    # a^l = f(z^l)\n",
    "    for l in range(1, nn.number_of_layers):\n",
    "        layer_l = nn.layers[l]\n",
    "        layer_prev = nn.layers[l - 1]\n",
    "        \n",
    "        # z1 = w^l dot a^(l-1)\n",
    "        z1 = matrix_dot(layer_l.weights, layer_prev.activations)\n",
    "        \n",
    "        # z2 = b^l dot 1, où 1 est un vecteur-ligne de taille minibatch \n",
    "        # => (nneurons, 1) x (1, minibatch_size) = (nneurons, minibatch_size)\n",
    "        ones = np.ones((1, nn.minibatch_size), dtype=np.float64)\n",
    "        z2 = matrix_dot(layer_l.biases, ones)\n",
    "        \n",
    "        # z^l\n",
    "        layer_l.z = matrix_sum(z1, z2)\n",
    "        \n",
    "        # a^l\n",
    "        layer_l.activations = matrix_function(layer_l.z, activation_function)\n",
    "\n",
    "def backward(nn, y, derivative_actfunct):\n",
    "    # L = index de la dernière couche\n",
    "    L = nn.number_of_layers - 1\n",
    "\n",
    "    # 1) Calcul de delta^L = (a^L - y) hadamard f'(z^L)\n",
    "    layer_L = nn.layers[L]\n",
    "    tmp = matrix_minus(layer_L.activations, y)  # a^L - y\n",
    "    dfzL = matrix_function(layer_L.z, derivative_actfunct)  # f'(z^L)\n",
    "    layer_L.delta = hadamard_product(tmp, dfzL)\n",
    "\n",
    "    # 2) Rétropropagation pour l= L..2\n",
    "    #    delta^(l-1) = (w^l)^T dot delta^l hadamard f'(z^(l-1))\n",
    "    for l in range(L, 1, -1):\n",
    "        layer_l     = nn.layers[l]\n",
    "        layer_lm1   = nn.layers[l - 1]  # l-1\n",
    "        w_l_transp  = matrix_transpose(layer_l.weights)\n",
    "        delta_tmp   = matrix_dot(w_l_transp, layer_l.delta)\n",
    "        dfz         = matrix_function(layer_lm1.z, derivative_actfunct)\n",
    "        layer_lm1.delta = hadamard_product(delta_tmp, dfz)\n",
    "\n",
    "    # 3) Mise à jour poids & biais\n",
    "    #    w^l = w^l - ( alpha / m ) * ( delta^l dot (a^(l-1))^T )\n",
    "    #    b^l = b^l - ( alpha / m ) * ( delta^l dot 1 )\n",
    "    for l in range(1, nn.number_of_layers):\n",
    "        layer_l     = nn.layers[l]\n",
    "        layer_lm1   = nn.layers[l - 1]\n",
    "        \n",
    "        a_lm1_transp = matrix_transpose(layer_lm1.activations)\n",
    "        \n",
    "        # gradient w\n",
    "        w1 = matrix_dot(layer_l.delta, a_lm1_transp)\n",
    "        w1 = matrix_scalar(w1, nn.alpha / nn.minibatch_size)\n",
    "        layer_l.weights = matrix_minus(layer_l.weights, w1)\n",
    "        \n",
    "        # gradient b\n",
    "        ones = np.ones((nn.minibatch_size, 1), dtype=np.float64)\n",
    "        b1 = matrix_dot(layer_l.delta, ones)  # shape (nneurons, 1)\n",
    "        b1 = matrix_scalar(b1, nn.alpha / nn.minibatch_size)\n",
    "        layer_l.biases = matrix_minus(layer_l.biases, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Fonctions utilisaires pour l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_minibatch(x, y, minibatch_idx, train_img, train_label):\n",
    "    \"\"\"\n",
    "    x -> shape (784, minibatch_size)\n",
    "    y -> shape (10,   minibatch_size)\n",
    "    minibatch_idx -> indices des exemples à mettre dans le batch\n",
    "    train_img et train_label sont les données entières\n",
    "    \"\"\"\n",
    "    batch_size = len(minibatch_idx)\n",
    "    for col, idx in enumerate(minibatch_idx):\n",
    "        # Remplir x (784, batch_size)\n",
    "        # Normaliser sur 255\n",
    "        x[:, col] = train_img[idx] / 255.0\n",
    "\n",
    "        # Remplir y (10, batch_size)\n",
    "        y[:, col] = 0.0\n",
    "        true_label = train_label[idx]\n",
    "        y[true_label, col] = 1.0\n",
    "\n",
    "def accuracy(nn, test_img, test_label, minibatch_size):\n",
    "    \"\"\"\n",
    "    Calcule l'accuracy en pourcentage sur l'ensemble des données test.\n",
    "    On effectue des mini-batches de taille 'minibatch_size'.\n",
    "    \"\"\"\n",
    "    ntest = test_img.shape[0]\n",
    "    good = 0\n",
    "    # Indices\n",
    "    idxs = zero_to_n(ntest)\n",
    "    \n",
    "    # Buffers pour x,y\n",
    "    x = alloc_matrix(784, minibatch_size)\n",
    "    y = alloc_matrix(10,   minibatch_size)\n",
    "    \n",
    "    # On itère par minibatch\n",
    "    nbatches = (ntest // minibatch_size) * minibatch_size\n",
    "    for i in range(0, nbatches, minibatch_size):\n",
    "        batch_indices = idxs[i:i+minibatch_size]\n",
    "        \n",
    "        populate_minibatch(x, y, batch_indices, test_img, test_label)\n",
    "        # On met x dans la couche d'entrée\n",
    "        set_input(nn, x)\n",
    "        # Forward\n",
    "        forward(nn, sigmoid)\n",
    "        \n",
    "        # Prédictions\n",
    "        # nn.layers[-1].activations -> shape (10, minibatch_size)\n",
    "        # On récupère l'indice de la classe la plus probable\n",
    "        predictions = np.argmax(nn.layers[-1].activations, axis=0)  # dimension= batch_size\n",
    "        # Comparaison\n",
    "        good_batch = np.sum(predictions == test_label[i:i+minibatch_size])\n",
    "        good += good_batch\n",
    "    \n",
    "    return (100.0 * good) / nbatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Execution Principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"DATA\"\n",
    "\n",
    "# 6.1) Lecture des données MNIST\n",
    "train_img = read_images(DATA_PATH + \"/train-images.idx3-ubyte\")\n",
    "train_label = read_labels(DATA_PATH + \"/train-labels.idx1-ubyte\")\n",
    "test_img = read_images(DATA_PATH + \"/t10k-images.idx3-ubyte\")\n",
    "test_label = read_labels(DATA_PATH + \"/t10k-labels.idx1-ubyte\")\n",
    "\n",
    "datasize = train_img.shape[0]\n",
    "ntest    = test_img.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting accuracy: 8.92\n"
     ]
    }
   ],
   "source": [
    "# 6.2) Création du réseau de neurones\n",
    "alpha = 0.05\n",
    "minibatch_size = 16\n",
    "number_of_layers = 3\n",
    "nneurons_per_layer = [784, 30, 10]  # 28*28 = 784\n",
    "nn = ANN(alpha, minibatch_size, number_of_layers, nneurons_per_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3) Buffers utiles pour l'entraînement\n",
    "shuffled_idx = zero_to_n(datasize)\n",
    "x = alloc_matrix(784, minibatch_size)\n",
    "y = alloc_matrix(10,   minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Acc: 8.92%: 100%|██████████| 3750/3750 [00:01<00:00, 2523.51it/s]\n",
      "Epoch 1 - Acc: 74.93%: 100%|██████████| 3750/3750 [00:01<00:00, 2456.30it/s]\n",
      "Epoch 2 - Acc: 86.84%: 100%|██████████| 3750/3750 [00:01<00:00, 2407.27it/s]\n",
      "Epoch 3 - Acc: 89.10%: 100%|██████████| 3750/3750 [00:01<00:00, 2258.73it/s]\n",
      "Epoch 4 - Acc: 90.11%: 100%|██████████| 3750/3750 [00:01<00:00, 2404.77it/s]\n",
      "Epoch 5 - Acc: 90.69%: 100%|██████████| 3750/3750 [00:01<00:00, 2302.67it/s]\n",
      "Epoch 6 - Acc: 91.25%: 100%|██████████| 3750/3750 [00:01<00:00, 2397.00it/s]\n",
      "Epoch 7 - Acc: 91.54%: 100%|██████████| 3750/3750 [00:01<00:00, 2346.00it/s]\n",
      "Epoch 8 - Acc: 91.83%: 100%|██████████| 3750/3750 [00:01<00:00, 2318.41it/s]\n",
      "Epoch 9 - Acc: 92.00%: 100%|██████████| 3750/3750 [00:01<00:00, 2295.91it/s]\n",
      "Epoch 10 - Acc: 92.23%: 100%|██████████| 3750/3750 [00:01<00:00, 2212.47it/s]\n",
      "Epoch 11 - Acc: 92.35%: 100%|██████████| 3750/3750 [00:01<00:00, 2320.72it/s]\n",
      "Epoch 12 - Acc: 92.60%: 100%|██████████| 3750/3750 [00:01<00:00, 2235.31it/s]\n",
      "Epoch 13 - Acc: 92.58%: 100%|██████████| 3750/3750 [00:01<00:00, 2277.74it/s]\n",
      "Epoch 14 - Acc: 92.71%: 100%|██████████| 3750/3750 [00:01<00:00, 2275.63it/s]\n",
      "Epoch 15 - Acc: 92.87%: 100%|██████████| 3750/3750 [00:01<00:00, 2307.23it/s]\n",
      "Epoch 16 - Acc: 92.95%: 100%|██████████| 3750/3750 [00:01<00:00, 2386.79it/s]\n",
      "Epoch 17 - Acc: 93.10%: 100%|██████████| 3750/3750 [00:01<00:00, 2243.51it/s]\n",
      "Epoch 18 - Acc: 93.24%: 100%|██████████| 3750/3750 [00:01<00:00, 2389.91it/s]\n",
      "Epoch 19 - Acc: 93.18%: 100%|██████████| 3750/3750 [00:01<00:00, 2278.23it/s]\n"
     ]
    }
   ],
   "source": [
    "acc_start = accuracy(nn, test_img, test_label, minibatch_size)\n",
    "print(\"Starting accuracy:\", acc_start)\n",
    "\n",
    "NEPOCHS = 20\n",
    "# 6.4) Boucle d'entraînement (10 itérations)\n",
    "# On calcule par exemple l'accuracy et la cross-entropy à chaque epoch.\n",
    "for epoch in range(NEPOCHS):\n",
    "    # On mélange les indices\n",
    "    shuffle(shuffled_idx, datasize)\n",
    "    \n",
    "    # On va parcourir l'ensemble du jeu d'entraînement par minibatch\n",
    "    nbatches = (datasize // minibatch_size) * minibatch_size\n",
    "    batch_iter = range(0, nbatches, minibatch_size)\n",
    "    \n",
    "    # Accumulateur pour la cross-entropy \n",
    "    ce_total = 0.0\n",
    "    n_train_batches = 0\n",
    "    \n",
    "    # Calculer l'accuracy actuelle\n",
    "    acc = accuracy(nn, test_img, test_label, minibatch_size)\n",
    "    desc = f'Epoch {epoch} - Acc: {acc:.2f}%'\n",
    "    \n",
    "    for i in tqdm(batch_iter, desc=desc):\n",
    "        batch_indices = shuffled_idx[i : i + minibatch_size]\n",
    "        populate_minibatch(x, y, batch_indices, train_img, train_label)\n",
    "        \n",
    "        set_input(nn, x)\n",
    "        forward(nn, sigmoid)\n",
    "        \n",
    "        # Récupérer la prédiction pour calculer la cross-entropy\n",
    "        y_pred = nn.layers[-1].activations  \n",
    "        ce_batch = cross_entropy_error(y_pred, y)\n",
    "        ce_total += ce_batch\n",
    "        n_train_batches += 1\n",
    "\n",
    "        backward(nn, y, dsigmoid)\n",
    "\n",
    "    # Moyenne cross-entropy de l'epoch\n",
    "    ce_mean = ce_total / n_train_batches\n",
    "    acc = accuracy(nn, test_img, test_label, minibatch_size)\n",
    "    \n",
    "    desc = f'Epoch {epoch} - Acc: {acc:.2f}%, CE: {ce_mean:.4f}'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECN_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
