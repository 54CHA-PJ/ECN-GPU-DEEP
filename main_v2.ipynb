{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numba as nb\n",
    "from numba import cuda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Lecture des fichiers MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uint32(byte_array):\n",
    "    \"\"\" Recompose un entier 32 bits à partir de 4 octets de poids fort à poids faible \"\"\"\n",
    "    return ((byte_array[0] << 24) \n",
    "          | (byte_array[1] << 16) \n",
    "          | (byte_array[2] <<  8) \n",
    "          | (byte_array[3] <<  0))\n",
    "\n",
    "def read_labels(filename):\n",
    "    \"\"\" Lit un fichier de labels MNIST \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        _ = f.read(4)  # Magic number (non utilisé)\n",
    "        n_bytes = f.read(4)\n",
    "        n = make_uint32(n_bytes)\n",
    "        labels = np.frombuffer(f.read(n), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "def read_images(filename):\n",
    "    \"\"\" Lit un fichier d'images MNIST \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        _ = f.read(4)  # Magic number (non utilisé)\n",
    "        n_bytes = f.read(4)\n",
    "        n = make_uint32(n_bytes)\n",
    "        row_bytes = f.read(4)\n",
    "        col_bytes = f.read(4)\n",
    "        rows = make_uint32(row_bytes)\n",
    "        cols = make_uint32(col_bytes)\n",
    "        images_raw = f.read(n * rows * cols)\n",
    "        images = np.frombuffer(images_raw, dtype=np.uint8)\n",
    "        images = images.reshape(n, rows * cols)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_to_n(n):\n",
    "    \"\"\" Crée un tableau [0, 1, 2, ..., n-1] \"\"\"\n",
    "    return np.arange(n, dtype=np.uint32)\n",
    "\n",
    "def shuffle(t, number_of_switch):\n",
    "    \"\"\" Mélange un tableau t aléatoirement, en réalisant 'number_of_switch' échanges \"\"\"\n",
    "    size = len(t)\n",
    "    for _ in range(number_of_switch):\n",
    "        x = np.random.randint(0, size)\n",
    "        y = np.random.randint(0, size)\n",
    "        tmp = t[x]\n",
    "        t[x] = t[y]\n",
    "        t[y] = tmp\n",
    "        \n",
    "def init_sigma(nneurons_prev):\n",
    "    return 1.0 / np.sqrt(nneurons_prev)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\" Fonction d'activation vectorisée \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    \"\"\" Dérivée de sigmoid vectorisée \"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1.0 - s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Fonctions Matricielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alloc_matrix(rows, columns):\n",
    "    # En C, on fait un malloc puis un tableau 1D rows*columns.\n",
    "    # En Python, on crée un np.ndarray de shape (rows, columns).\n",
    "    return np.zeros((rows, columns), dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.3. Version Parallèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matrix_dot_cuda(m1, m2, res, nrow1, ncol1, ncol2):\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < nrow1 and col < ncol2:\n",
    "        tmp = 0.0\n",
    "        for k in range(ncol1):\n",
    "            tmp += m1[row, k] * m2[k, col]\n",
    "        res[row, col] = tmp\n",
    "\n",
    "@cuda.jit\n",
    "def matrix_sum_cuda(m1, m2, res, nrow, ncol):\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < nrow and col < ncol:\n",
    "        res[row, col] = m1[row, col] + m2[row, col]\n",
    "\n",
    "@cuda.jit\n",
    "def matrix_minus_cuda(m1, m2, res, nrow, ncol):\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < nrow and col < ncol:\n",
    "        res[row, col] = m1[row, col] - m2[row, col]\n",
    "\n",
    "@cuda.jit\n",
    "def hadamard_product_cuda(m1, m2, res, nrow, ncol):\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < nrow and col < ncol:\n",
    "        res[row, col] = m1[row, col] * m2[row, col]\n",
    "\n",
    "@cuda.jit\n",
    "def matrix_transpose_cuda(m, res, nrow, ncol):\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < nrow and col < ncol:\n",
    "        res[col, row] = m[row, col]\n",
    "\n",
    "@cuda.jit\n",
    "def matrix_scalar_cuda(m, res, scalar, nrow, ncol):\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < nrow and col < ncol:\n",
    "        res[row, col] = m[row, col] * scalar\n",
    "\n",
    "@cuda.jit\n",
    "def matrix_memcpy_cuda(dest, src, nrow, ncol):\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < nrow and col < ncol:\n",
    "        dest[row, col] = src[row, col]\n",
    "        \n",
    "@cuda.jit\n",
    "def matrix_function_id_cuda(m_in, m_out, nrow, ncol, func_id):\n",
    "    \"\"\"\n",
    "    Applique la fonction correspondant à func_id :\n",
    "      0 -> exp\n",
    "      1 -> sigmoid\n",
    "      2 -> dsigmoid\n",
    "      ...\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < nrow and col < ncol:\n",
    "        val = m_in[row, col]\n",
    "        if func_id == 0:\n",
    "            result = math.exp(val)\n",
    "        elif func_id == 1:\n",
    "            result = 1.0 / (1.0 + math.exp(-val))\n",
    "        elif func_id == 2:\n",
    "            s = 1.0 / (1.0 + math.exp(-val))\n",
    "            result = s * (1.0 - s)\n",
    "        else:\n",
    "            result = val\n",
    "        m_out[row, col] = result\n",
    "\n",
    "def matrix_dot_parallel(m1, m2):\n",
    "    nrow1, ncol1 = m1.shape; nrow2, ncol2 = m2.shape; \n",
    "    if ncol1 != nrow2: raise ValueError(\"Incompatible matrix dimensions\")\n",
    "    m1_gpu = cuda.to_device(m1)\n",
    "    m2_gpu = cuda.to_device(m2)\n",
    "    res_gpu = cuda.device_array((nrow1, ncol2), dtype=np.float64)\n",
    "    threads_per_block = (16,16)\n",
    "    blocks_per_grid = (math.ceil(ncol2/16), math.ceil(nrow1/16))\n",
    "    matrix_dot_cuda[blocks_per_grid, threads_per_block](m1_gpu, m2_gpu, res_gpu, nrow1, ncol1, ncol2)\n",
    "    return res_gpu.copy_to_host()\n",
    "\n",
    "def matrix_sum_parallel(m1, m2):\n",
    "    nrow, ncol = m1.shape\n",
    "    m1_gpu = cuda.to_device(m1); m2_gpu = cuda.to_device(m2)\n",
    "    res_gpu = cuda.device_array((nrow, ncol), dtype=np.float64)\n",
    "    threads_per_block = (16,16)\n",
    "    blocks_per_grid = (math.ceil(nrow/16), math.ceil(ncol/16))\n",
    "    matrix_sum_cuda[blocks_per_grid, threads_per_block](m1_gpu, m2_gpu, res_gpu, nrow, ncol)\n",
    "    return res_gpu.copy_to_host()\n",
    "\n",
    "def matrix_minus_parallel(m1, m2):\n",
    "    nrow, ncol = m1.shape\n",
    "    m1_gpu = cuda.to_device(m1); m2_gpu = cuda.to_device(m2)\n",
    "    res_gpu = cuda.device_array((nrow, ncol), dtype=np.float64)\n",
    "    threads_per_block = (16,16)\n",
    "    blocks_per_grid = (math.ceil(nrow/16), math.ceil(ncol/16))\n",
    "    matrix_minus_cuda[blocks_per_grid, threads_per_block](m1_gpu, m2_gpu, res_gpu, nrow, ncol)\n",
    "    return res_gpu.copy_to_host()\n",
    "\n",
    "def hadamard_product_parallel(m1, m2):\n",
    "    nrow, ncol = m1.shape\n",
    "    m1_gpu = cuda.to_device(m1); m2_gpu = cuda.to_device(m2)\n",
    "    res_gpu = cuda.device_array((nrow, ncol), dtype=np.float64)\n",
    "    threads_per_block = (16,16)\n",
    "    blocks_per_grid = (math.ceil(nrow/16), math.ceil(ncol/16))\n",
    "    hadamard_product_cuda[blocks_per_grid, threads_per_block](m1_gpu, m2_gpu, res_gpu, nrow, ncol)\n",
    "    return res_gpu.copy_to_host()\n",
    "\n",
    "def matrix_transpose_parallel(m):\n",
    "    nrow, ncol = m.shape\n",
    "    m_gpu = cuda.to_device(m)\n",
    "    res_gpu = cuda.device_array((ncol, nrow), dtype=np.float64)\n",
    "    threads_per_block = (16,16)\n",
    "    blocks_per_grid = (math.ceil(nrow/16), math.ceil(ncol/16))\n",
    "    matrix_transpose_cuda[blocks_per_grid, threads_per_block](m_gpu, res_gpu, nrow, ncol)\n",
    "    return res_gpu.copy_to_host()\n",
    "\n",
    "def matrix_scalar_parallel(m, s):\n",
    "    nrow, ncol = m.shape\n",
    "    m_gpu = cuda.to_device(m)\n",
    "    res_gpu = cuda.device_array((nrow, ncol), dtype=np.float64)\n",
    "    threads_per_block = (16,16)\n",
    "    blocks_per_grid = (math.ceil(nrow/16), math.ceil(ncol/16))\n",
    "    matrix_scalar_cuda[blocks_per_grid, threads_per_block](m_gpu, res_gpu, s, nrow, ncol)\n",
    "    return res_gpu.copy_to_host()\n",
    "\n",
    "def matrix_memcpy_parallel(dest, src):\n",
    "    nrow, ncol = src.shape\n",
    "    dest_gpu = cuda.to_device(dest); src_gpu = cuda.to_device(src)\n",
    "    threads_per_block = (16,16)\n",
    "    blocks_per_grid = (math.ceil(nrow/16), math.ceil(ncol/16))\n",
    "    matrix_memcpy_cuda[blocks_per_grid, threads_per_block](dest_gpu, src_gpu, nrow, ncol)\n",
    "    dest[:] = dest_gpu.copy_to_host()\n",
    "\n",
    "def matrix_function_parallel(m1, func_name):\n",
    "    func_id = 0 if func_name == \"exp\" else \\\n",
    "              1 if func_name == \"sigmoid\" else \\\n",
    "              2 if func_name == \"dsigmoid\" else \\\n",
    "              None\n",
    "    if func_id is None: raise ValueError(f\"Unknown function ID : {func_name}\")\n",
    "    nrow, ncol = m1.shape\n",
    "    m1_gpu = cuda.to_device(m1)\n",
    "    res_gpu = cuda.device_array((nrow, ncol), dtype=np.float64)\n",
    "    threads_per_block = (16,16)\n",
    "    blocks_per_grid = (math.ceil(nrow/16), math.ceil(ncol/16))\n",
    "    matrix_function_id_cuda[blocks_per_grid, threads_per_block](m1_gpu, res_gpu, nrow, ncol, func_id)\n",
    "    return res_gpu.copy_to_host()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Réseau de Neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, layer_number, number_of_neurons, nneurons_previous_layer, minibatch_size):\n",
    "        self.number_of_neurons = number_of_neurons\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.activations = alloc_matrix(number_of_neurons, minibatch_size)\n",
    "        self.z           = alloc_matrix(number_of_neurons, minibatch_size)\n",
    "        self.delta       = alloc_matrix(number_of_neurons, minibatch_size)\n",
    "        self.weights     = alloc_matrix(number_of_neurons, nneurons_previous_layer)\n",
    "        self.biases      = alloc_matrix(number_of_neurons, 1)\n",
    "        \n",
    "        if layer_number > 0:\n",
    "            self.init_weight(nneurons_previous_layer)\n",
    "    \n",
    "    def init_weight(self, nneurons_prev):\n",
    "        sigma = init_sigma(nneurons_prev)\n",
    "        r, c = self.weights.shape\n",
    "        self.weights = np.random.normal(0.0, sigma, size=(r, c))\n",
    "\n",
    "class ANN:\n",
    "    def __init__(self, alpha, minibatch_size, number_of_layers, nneurons_per_layer):\n",
    "        self.alpha = alpha\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.number_of_layers = number_of_layers\n",
    "        self.layers = []\n",
    "        for i in range(number_of_layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(\n",
    "                    Layer(i, nneurons_per_layer[i], \n",
    "                          nneurons_per_layer[i],  \n",
    "                          minibatch_size)\n",
    "                )\n",
    "            else:\n",
    "                self.layers.append(\n",
    "                    Layer(i, nneurons_per_layer[i],\n",
    "                          nneurons_per_layer[i-1],\n",
    "                          minibatch_size)\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_input_parallel(nn, input_matrix):\n",
    "    matrix_memcpy_parallel(nn.layers[0].activations, input_matrix)\n",
    "\n",
    "def forward_parallel(nn):\n",
    "    for l in range(1, nn.number_of_layers):\n",
    "        layer_l = nn.layers[l]\n",
    "        layer_prev = nn.layers[l-1]\n",
    "        z1 = matrix_dot_parallel(layer_l.weights, layer_prev.activations)\n",
    "        ones = np.ones((1, nn.minibatch_size), dtype=np.float64)\n",
    "        z2 = matrix_dot_parallel(layer_l.biases, ones)\n",
    "        layer_l.z = matrix_sum_parallel(z1, z2)\n",
    "        layer_l.activations = matrix_function_parallel(layer_l.z, \"sigmoid\")\n",
    "\n",
    "def backward_parallel(nn, y):\n",
    "    L = nn.number_of_layers - 1\n",
    "    layer_L = nn.layers[L]\n",
    "    tmp = matrix_minus_parallel(layer_L.activations, y)\n",
    "    dfzL = matrix_function_parallel(layer_L.z, \"dsigmoid\")\n",
    "    layer_L.delta = hadamard_product_parallel(tmp, dfzL)\n",
    "    for l in range(L, 1, -1):\n",
    "        layer_l = nn.layers[l]\n",
    "        layer_lm1 = nn.layers[l-1]\n",
    "        w_l_transp = matrix_transpose_parallel(layer_l.weights)\n",
    "        delta_tmp = matrix_dot_parallel(w_l_transp, layer_l.delta)\n",
    "        dfz = matrix_function_parallel(layer_lm1.z, \"dsigmoid\")\n",
    "        layer_lm1.delta = hadamard_product_parallel(delta_tmp, dfz)\n",
    "    for l in range(1, nn.number_of_layers):\n",
    "        layer_l = nn.layers[l]\n",
    "        layer_lm1 = nn.layers[l-1]\n",
    "        a_lm1_transp = matrix_transpose_parallel(layer_lm1.activations)\n",
    "        w1 = matrix_dot_parallel(layer_l.delta, a_lm1_transp)\n",
    "        w1 = matrix_scalar_parallel(w1, nn.alpha / nn.minibatch_size)\n",
    "        layer_l.weights = matrix_minus_parallel(layer_l.weights, w1)\n",
    "        ones = np.ones((nn.minibatch_size, 1), dtype=np.float64)\n",
    "        b1 = matrix_dot_parallel(layer_l.delta, ones)\n",
    "        b1 = matrix_scalar_parallel(b1, nn.alpha / nn.minibatch_size)\n",
    "        layer_l.biases = matrix_minus_parallel(layer_l.biases, b1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Fonctions d'entraînement (Version Parallèle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_parallel(x, y, minibatch_idx, train_img, train_label):\n",
    "    \"\"\"\n",
    "    Remplit les matrices x et y avec le mini-batch.\n",
    "    x -> shape (784, minibatch_size)\n",
    "    y -> shape (10, minibatch_size)\n",
    "    \"\"\"\n",
    "    x_batch = train_img[minibatch_idx].astype(np.float64) / 255.0\n",
    "    matrix_memcpy_parallel(x, x_batch.T)  # Transpose pour avoir (784, batch_size)\n",
    "    y.fill(0.0)\n",
    "    indices = train_label[minibatch_idx]\n",
    "    y[indices, np.arange(len(minibatch_idx))] = 1.0\n",
    "\n",
    "def accuracy_parallel(nn, test_img, test_label, minibatch_size):\n",
    "    \"\"\"\n",
    "    Compute the accuracy (%) on the test set using parallel matrix operations.\n",
    "    \"\"\"\n",
    "    test_size = test_img.shape[0]\n",
    "    nbatches = (test_size // minibatch_size) * minibatch_size\n",
    "    correct = 0\n",
    "    for i in range(0, nbatches, minibatch_size):\n",
    "        batch_indices = np.arange(i, i + minibatch_size)\n",
    "        x = test_img[batch_indices].T.astype(np.float64) / 255.0\n",
    "        set_input_parallel(nn, x)\n",
    "        forward_parallel(nn)\n",
    "        preds = np.argmax(nn.layers[-1].activations, axis=0)\n",
    "        correct += np.sum(preds == test_label[batch_indices])\n",
    "    return (100.0 * correct) / nbatches\n",
    "\n",
    "def cross_entropy_parallel(y_pred, y_true, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy error for a mini-batch using parallel operations.\n",
    "    \"\"\"\n",
    "    y_pred = np.clip(y_pred, eps, 1.0 - eps)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### premiere amelioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_parallel_v2(nn):\n",
    "    \"\"\"\n",
    "    We assume nn.layers[0].activations is already set (by set_input_parallel).\n",
    "    This function simply propagates forward through all layers.\n",
    "    \"\"\"\n",
    "    for l in range(1, nn.number_of_layers):\n",
    "        layer_l    = nn.layers[l]\n",
    "        layer_prev = nn.layers[l-1]\n",
    "        # Z1 = W * A_{l-1}\n",
    "        Z1 = matrix_dot_parallel(layer_l.weights, layer_prev.activations)\n",
    "        # Z2 = bias repeated across columns\n",
    "        ones = np.ones((1, Z1.shape[1]), dtype=np.float64)\n",
    "        Z2   = matrix_dot_parallel(layer_l.biases, ones)\n",
    "        # Z = Z1 + Z2\n",
    "        layer_l.z = matrix_sum_parallel(Z1, Z2)\n",
    "        # A_l = sigmoid(Z)\n",
    "        layer_l.activations = matrix_function_parallel(layer_l.z, \"sigmoid\")\n",
    "    # final activations are now in nn.layers[-1].activations\n",
    "\n",
    "def accuracy_parallel_v2(nn, test_img, test_label, minibatch_size, n_sub_batches=4):\n",
    "    \"\"\"\n",
    "    Compute accuracy in parallel, but group multiple mini‐batches\n",
    "    into one big batch for better GPU usage.\n",
    "    \"\"\"\n",
    "    test_size = test_img.shape[0]\n",
    "    nbatches = (test_size // minibatch_size) * minibatch_size\n",
    "    correct = 0\n",
    "    for i in range(0, nbatches, minibatch_size * n_sub_batches):\n",
    "        # how many sub‐batches can we handle this round?\n",
    "        actual_subbatches = min(n_sub_batches, (nbatches - i) // minibatch_size)\n",
    "        if actual_subbatches == 0:\n",
    "            break\n",
    "        total_cols = minibatch_size * actual_subbatches\n",
    "        big_data   = np.zeros((nn.layers[0].number_of_neurons, total_cols), dtype=np.float64)\n",
    "        big_labels = np.zeros(total_cols, dtype=test_label.dtype)\n",
    "        # pack sub‐batches into 'big_data'\n",
    "        for sb in range(actual_subbatches):\n",
    "            start_idx = i + sb * minibatch_size\n",
    "            end_idx   = start_idx + minibatch_size\n",
    "            batch_indices = np.arange(start_idx, end_idx)\n",
    "            x_sub = test_img[batch_indices].T.astype(np.float64) / 255.0\n",
    "            col_start = sb * minibatch_size\n",
    "            col_end   = col_start + minibatch_size\n",
    "            big_data[:, col_start:col_end]  = x_sub\n",
    "            big_labels[col_start:col_end]   = test_label[batch_indices]\n",
    "        # Forward pass over big_data (just set it to layer 0, then forward)\n",
    "        nn.layers[0].activations = big_data\n",
    "        forward_parallel_v2(nn)\n",
    "        # predictions for all sub‐batches at once\n",
    "        preds = np.argmax(nn.layers[-1].activations, axis=0)\n",
    "        correct += np.sum(preds == big_labels)\n",
    "    return (100.0 * correct) / nbatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deuxieme amelioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_minibatch_size(minimum=16, maximum=1024):\n",
    "    \"\"\"\n",
    "    Naive heuristic: pick a batch size that's a power of 2 between 'minimum' and 'maximum'\n",
    "    so that we can spawn enough threads to fill the GPU. \n",
    "    This doesn't do advanced memory checks. It's purely a simple guess based on SM count.\n",
    "    \"\"\"\n",
    "    device = cuda.get_current_device()\n",
    "    mp_count = device.MULTIPROCESSOR_COUNT      # Number of SMs\n",
    "    warp_size = device.WARP_SIZE               # Usually 32\n",
    "    # Suppose we want to ensure at least (mp_count * some_factor) threads \n",
    "    # are in flight per kernel. We'll pick e.g. 8 warps per SM => 8 * warp_size = 256 threads each SM\n",
    "    desired_threads_per_sm = 8 * warp_size      # 8 warps per SM\n",
    "    total_desired_threads = mp_count * desired_threads_per_sm\n",
    "    \n",
    "    # Now we guess a batch_size that, when multiplied by (some dimension) \n",
    "    # or some expected usage, doesn't trivially under‐utilize the GPU.\n",
    "    # For instance, if the main dimension is 784, we might want batch_size * 784 ~ total_desired_threads\n",
    "    # We'll keep it simple: batch_size >= total_desired_threads / 784 \n",
    "    approx_needed = math.ceil(total_desired_threads / 784)\n",
    "    \n",
    "    # Round that up to the nearest power of 2\n",
    "    # but clamp it between 'minimum' and 'maximum'\n",
    "    candidate = 1\n",
    "    while candidate < approx_needed:\n",
    "        candidate <<= 1  # candidate *= 2\n",
    "    if candidate < minimum:\n",
    "        candidate = minimum\n",
    "    if candidate > maximum:\n",
    "        candidate = maximum\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_n_sub_batches(nn, test_size, default=4, max_sub=16):\n",
    "    \"\"\"\n",
    "    Naive heuristic to pick a sub-batch count for accuracy checks.\n",
    "    We want: sub-batches <= max_sub, and (sub-batches * minibatch_size) <= test_size\n",
    "    We'll also consider the GPU SM count to see if it's large or small.\n",
    "    \"\"\"\n",
    "    device = cuda.get_current_device()\n",
    "    mp_count = device.MULTIPROCESSOR_COUNT  # how many SMs\n",
    "    # If we have a lot of SMs, we might want more sub-batches to keep the GPU busier.\n",
    "    # Let's do a simple approach: if mp_count >= 20, we'll try 8 by default, else 4.\n",
    "    suggested = 8 if mp_count >= 20 else default\n",
    "\n",
    "    # But clamp it so that sub-batches * minibatch_size isn't bigger than test_size\n",
    "    max_possible = test_size // nn.minibatch_size\n",
    "    final_sub = min(suggested, max_possible, max_sub)\n",
    "    if final_sub < 1:\n",
    "        final_sub = 1\n",
    "\n",
    "    print(f\"Guessed n_sub_batches = {final_sub} based on GPU SMs={mp_count} and test_size={test_size}\")\n",
    "    return final_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### troisieme amelioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matrix_dot_bias_activation_cuda(W, A, B, out, nrowW, ncolW, ncolA):\n",
    "    \"\"\"\n",
    "    out[row,col] = sigmoid( sum_{k in 0..(ncolW-1)} W[row,k]*A[k,col] + B[row,0] )\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < nrowW and col < ncolA:\n",
    "        # do the dot\n",
    "        tmp = 0.0\n",
    "        for k in range(ncolW):\n",
    "            tmp += W[row, k] * A[k, col]\n",
    "        # add bias\n",
    "        tmp += B[row, 0]\n",
    "        # apply sigmoid\n",
    "        val = 1.0 / (1.0 + math.exp(-tmp))\n",
    "        out[row, col] = val\n",
    "\n",
    "def matrix_dot_bias_activation_parallel(W, A, B):\n",
    "    \"\"\"\n",
    "    Single GPU kernel that does:\n",
    "       out[row,col] = sigmoid( W[row,:] dot A[:,col] + B[row,0] )\n",
    "    Returns out with shape (W.shape[0], A.shape[1]).\n",
    "    \"\"\"\n",
    "    nrowW, ncolW = W.shape  # W is shape (nrowW, ncolW)\n",
    "    nrowA, ncolA = A.shape  # A is shape (nrowA, ncolA)\n",
    "    if ncolW != nrowA:\n",
    "        raise ValueError(\"Incompatible shapes for matrix-dot-bias-activation!\")\n",
    "    nrowB, ncolB = B.shape\n",
    "    if not (nrowB == nrowW and ncolB == 1):\n",
    "        raise ValueError(\"Bias shape mismatch! B should be (nrowW, 1)\")\n",
    "\n",
    "    # allocate output\n",
    "    out_gpu = cuda.device_array((nrowW, ncolA), dtype=np.float64)\n",
    "\n",
    "    # copy inputs to GPU\n",
    "    W_gpu = cuda.to_device(W)\n",
    "    A_gpu = cuda.to_device(A)\n",
    "    B_gpu = cuda.to_device(B)\n",
    "\n",
    "    threads_per_block = (16, 16)\n",
    "    blocks_per_grid = (math.ceil(ncolA / 16), math.ceil(nrowW / 16))\n",
    "\n",
    "    matrix_dot_bias_activation_cuda[blocks_per_grid, threads_per_block](\n",
    "        W_gpu, A_gpu, B_gpu, out_gpu, nrowW, ncolW, ncolA\n",
    "    )\n",
    "\n",
    "    return out_gpu.copy_to_host()\n",
    "\n",
    "def forward_parallel_v3(nn):\n",
    "    \"\"\"\n",
    "    New forward pass that merges (dot + bias + sigmoid) \n",
    "    into one GPU kernel for each layer.\n",
    "    \"\"\"\n",
    "    for l in range(1, nn.number_of_layers):\n",
    "        layer_l    = nn.layers[l]\n",
    "        layer_prev = nn.layers[l-1]\n",
    "        # single call merges the 3 steps:\n",
    "        layer_l.activations = matrix_dot_bias_activation_parallel(\n",
    "            layer_l.weights, \n",
    "            layer_prev.activations,\n",
    "            layer_l.biases\n",
    "        )\n",
    "        # we don't need separate layer_l.z if we want just the final activation;\n",
    "        # but if you want to keep z for backprop, you can store it as well:\n",
    "        layer_l.z = layer_l.activations.copy()\n",
    "\n",
    "\n",
    "def accuracy_parallel_v3(nn, test_img, test_label, minibatch_size, n_sub_batches=4):\n",
    "    \"\"\"\n",
    "    Compute accuracy in parallel, but group multiple mini‐batches\n",
    "    into one big batch for better GPU usage.\n",
    "    \"\"\"\n",
    "    test_size = test_img.shape[0]\n",
    "    nbatches = (test_size // minibatch_size) * minibatch_size\n",
    "    correct = 0\n",
    "    for i in range(0, nbatches, minibatch_size * n_sub_batches):\n",
    "        # how many sub‐batches can we handle this round?\n",
    "        actual_subbatches = min(n_sub_batches, (nbatches - i) // minibatch_size)\n",
    "        if actual_subbatches == 0:\n",
    "            break\n",
    "        total_cols = minibatch_size * actual_subbatches\n",
    "        big_data   = np.zeros((nn.layers[0].number_of_neurons, total_cols), dtype=np.float64)\n",
    "        big_labels = np.zeros(total_cols, dtype=test_label.dtype)\n",
    "        # pack sub‐batches into 'big_data'\n",
    "        for sb in range(actual_subbatches):\n",
    "            start_idx = i + sb * minibatch_size\n",
    "            end_idx   = start_idx + minibatch_size\n",
    "            batch_indices = np.arange(start_idx, end_idx)\n",
    "            x_sub = test_img[batch_indices].T.astype(np.float64) / 255.0\n",
    "            col_start = sb * minibatch_size\n",
    "            col_end   = col_start + minibatch_size\n",
    "            big_data[:, col_start:col_end]  = x_sub\n",
    "            big_labels[col_start:col_end]   = test_label[batch_indices]\n",
    "        # Forward pass over big_data (just set it to layer 0, then forward)\n",
    "        nn.layers[0].activations = big_data\n",
    "        forward_parallel_v3(nn)\n",
    "        # predictions for all sub‐batches at once\n",
    "        preds = np.argmax(nn.layers[-1].activations, axis=0)\n",
    "        correct += np.sum(preds == big_labels)\n",
    "    return (100.0 * correct) / nbatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Lecture des données MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de données d'entraînement : 60000\n",
      "Nombre de données de test : 10000\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"DATA\"\n",
    "\n",
    "train_img = read_images(os.path.join(DATA_PATH, \"train-images.idx3-ubyte\"))\n",
    "train_label = read_labels(os.path.join(DATA_PATH, \"train-labels.idx1-ubyte\"))\n",
    "test_img = read_images(os.path.join(DATA_PATH, \"t10k-images.idx3-ubyte\"))\n",
    "test_label = read_labels(os.path.join(DATA_PATH, \"t10k-labels.idx1-ubyte\"))\n",
    "\n",
    "train_size = train_img.shape[0]\n",
    "test_size = test_img.shape[0]\n",
    "print(f\"Nombre de données d'entraînement : {train_size}\")\n",
    "print(f\"Nombre de données de test : {test_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Entraînement du réseau (Version Parallèle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessed minibatch_size = 16\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "minibatch_size = guess_minibatch_size(minimum=16, maximum=1024)\n",
    "print(\"Guessed minibatch_size =\", minibatch_size)\n",
    "\n",
    "number_of_layers = 3\n",
    "nneurons_per_layer = [784, 30, 10]  # 28*28 = 784\n",
    "nn = ANN(alpha, minibatch_size, number_of_layers, nneurons_per_layer)\n",
    "\n",
    "shuffled_idx = zero_to_n(train_size)\n",
    "x = alloc_matrix(784, minibatch_size)\n",
    "y = alloc_matrix(10, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 8 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "Epoch 0 - Acc: 9.53%:   0%|          | 0/3750 [00:00<?, ?it/s]c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 49 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 49 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 98 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 98 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\sacha\\miniconda3\\envs\\ECN_GPU\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "Epoch 0 - Acc: 9.53%: 100%|██████████| 3750/3750 [02:36<00:00, 23.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Acc: 10.93%, CE: 2.2274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Acc: 10.89%: 100%|██████████| 3750/3750 [02:12<00:00, 28.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Acc: 12.05%, CE: 2.2343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Acc: 12.07%: 100%|██████████| 3750/3750 [02:37<00:00, 23.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Acc: 12.07%, CE: 3.7070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Acc: 11.92%: 100%|██████████| 3750/3750 [02:35<00:00, 24.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Acc: 13.19%, CE: 2.1315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Acc: 13.07%: 100%|██████████| 3750/3750 [02:23<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Acc: 14.27%, CE: 2.1158\n",
      "Final Model : Accuracy = 14.34%, Cross-Entropy Error = 2.1599\n"
     ]
    }
   ],
   "source": [
    "NEPOCHS = 5\n",
    "for epoch in range(NEPOCHS):\n",
    "    shuffle(shuffled_idx, train_size)\n",
    "    nbatches = (train_size // minibatch_size) * minibatch_size\n",
    "    batch_iter = range(0, nbatches, minibatch_size)\n",
    "    ce_total = 0.0\n",
    "    n_train_batches = 0\n",
    "    acc = accuracy_parallel_v3(nn, test_img, test_label, minibatch_size)\n",
    "    desc = f'Epoch {epoch} - Acc: {acc:.2f}%'\n",
    "    for i in tqdm(batch_iter, desc=desc):\n",
    "        batch_indices = shuffled_idx[i : i + minibatch_size]\n",
    "        populate_parallel(x, y, batch_indices, train_img, train_label)\n",
    "        set_input_parallel(nn, x)\n",
    "        forward_parallel_v3(nn)\n",
    "        y_pred = nn.layers[-1].activations\n",
    "        ce_batch = cross_entropy_parallel(y_pred, y)\n",
    "        ce_total += ce_batch\n",
    "        n_train_batches += 1\n",
    "        backward_parallel(nn, y)\n",
    "    ce_mean = ce_total / n_train_batches\n",
    "    acc = accuracy_parallel_v3(nn, test_img, test_label, minibatch_size)\n",
    "    print(f'Epoch {epoch} - Acc: {acc:.2f}%, CE: {ce_mean:.4f}')\n",
    "\n",
    "acc_end = accuracy_parallel_v3(nn, test_img, test_label, minibatch_size)\n",
    "ce_end = cross_entropy_parallel(y_pred, y)\n",
    "print(\"Final Model : Accuracy = {:.2f}%, Cross-Entropy Error = {:.4f}\".format(acc_end, ce_end))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECN_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
